# Performance Regression Detection Workflow for Ladybird
# Compares benchmarks between PR and base branch

name: Performance Check

on:
  pull_request:
    types: [opened, synchronize, reopened]
  workflow_dispatch:

concurrency:
  group: perf-${{ github.ref }}
  cancel-in-progress: true

env:
  BENCHMARK_ITERATIONS: 5
  REGRESSION_THRESHOLD: 5  # Fail if >5% regression

jobs:
  benchmark-pr:
    name: Benchmark PR Branch
    runs-on: ubuntu-24.04
    timeout-minutes: 60

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.sha }}

      - name: Install Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            gcc-14 \
            g++-14 \
            cmake \
            ninja-build

      - name: Set up vcpkg
        run: |
          git clone https://github.com/microsoft/vcpkg.git Build/vcpkg
          ./Build/vcpkg/bootstrap-vcpkg.sh

      - name: Restore Build Cache
        uses: actions/cache@v4
        with:
          path: |
            .ccache
            Build/caches/vcpkg-binary-cache
          key: perf-pr-${{ github.sha }}
          restore-keys: perf-pr-

      - name: Configure Build (Optimized)
        env:
          CC: gcc-14
          CXX: g++-14
          VCPKG_ROOT: ${{ github.workspace }}/Build/vcpkg
        run: |
          cmake --preset Release \
            -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_CXX_FLAGS="-O3 -march=native -DNDEBUG"

      - name: Build
        run: cmake --build Build --parallel $(nproc)

      - name: Warm Up System
        run: |
          # Run benchmarks once to warm up caches
          ./Build/release/bin/js-benchmark-suite --iterations 1 || true

      - name: Run JavaScript Benchmarks
        run: |
          ./Build/release/bin/js-benchmark-suite \
            --iterations ${{ env.BENCHMARK_ITERATIONS }} \
            --output pr-js-benchmarks.json

      - name: Run Layout Benchmarks
        run: |
          ./Build/release/bin/layout-benchmark-suite \
            --iterations ${{ env.BENCHMARK_ITERATIONS }} \
            --output pr-layout-benchmarks.json

      - name: Run Page Load Benchmarks
        run: |
          ./Build/release/bin/Ladybird \
            --run-benchmarks \
            --benchmark-output pr-page-load-benchmarks.json

      - name: Measure Binary Size
        run: |
          du -h Build/release/bin/Ladybird > pr-binary-size.txt
          du -h Build/release/bin/WebContent >> pr-binary-size.txt
          du -h Build/release/bin/RequestServer >> pr-binary-size.txt

      - name: Upload PR Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: pr-benchmarks
          path: |
            pr-js-benchmarks.json
            pr-layout-benchmarks.json
            pr-page-load-benchmarks.json
            pr-binary-size.txt

  benchmark-base:
    name: Benchmark Base Branch
    runs-on: ubuntu-24.04
    timeout-minutes: 60

    steps:
      - name: Checkout Base Branch
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.base.sha }}

      - name: Install Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            gcc-14 \
            g++-14 \
            cmake \
            ninja-build

      - name: Set up vcpkg
        run: |
          git clone https://github.com/microsoft/vcpkg.git Build/vcpkg
          ./Build/vcpkg/bootstrap-vcpkg.sh

      - name: Restore Build Cache
        uses: actions/cache@v4
        with:
          path: |
            .ccache
            Build/caches/vcpkg-binary-cache
          key: perf-base-${{ github.event.pull_request.base.sha }}
          restore-keys: perf-base-

      - name: Configure Build (Optimized)
        env:
          CC: gcc-14
          CXX: g++-14
          VCPKG_ROOT: ${{ github.workspace }}/Build/vcpkg
        run: |
          cmake --preset Release \
            -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_CXX_FLAGS="-O3 -march=native -DNDEBUG"

      - name: Build
        run: cmake --build Build --parallel $(nproc)

      - name: Warm Up System
        run: |
          ./Build/release/bin/js-benchmark-suite --iterations 1 || true

      - name: Run JavaScript Benchmarks
        run: |
          ./Build/release/bin/js-benchmark-suite \
            --iterations ${{ env.BENCHMARK_ITERATIONS }} \
            --output base-js-benchmarks.json

      - name: Run Layout Benchmarks
        run: |
          ./Build/release/bin/layout-benchmark-suite \
            --iterations ${{ env.BENCHMARK_ITERATIONS }} \
            --output base-layout-benchmarks.json

      - name: Run Page Load Benchmarks
        run: |
          ./Build/release/bin/Ladybird \
            --run-benchmarks \
            --benchmark-output base-page-load-benchmarks.json

      - name: Measure Binary Size
        run: |
          du -h Build/release/bin/Ladybird > base-binary-size.txt
          du -h Build/release/bin/WebContent >> base-binary-size.txt
          du -h Build/release/bin/RequestServer >> base-binary-size.txt

      - name: Upload Base Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: base-benchmarks
          path: |
            base-js-benchmarks.json
            base-layout-benchmarks.json
            base-page-load-benchmarks.json
            base-binary-size.txt

  compare-performance:
    name: Compare and Report
    runs-on: ubuntu-latest
    needs: [benchmark-pr, benchmark-base]
    permissions:
      pull-requests: write

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Download PR Benchmarks
        uses: actions/download-artifact@v4
        with:
          name: pr-benchmarks
          path: pr/

      - name: Download Base Benchmarks
        uses: actions/download-artifact@v4
        with:
          name: base-benchmarks
          path: base/

      - name: Install Analysis Tools
        run: |
          pip install numpy scipy matplotlib

      - name: Compare Benchmarks
        id: comparison
        run: |
          python3 scripts/compare_benchmarks.py \
            --base base/ \
            --pr pr/ \
            --threshold ${{ env.REGRESSION_THRESHOLD }} \
            --output comparison-report.json \
            --markdown comparison-report.md

          # Check if there are regressions
          if grep -q "❌" comparison-report.md; then
            echo "has_regressions=true" >> $GITHUB_OUTPUT
          else
            echo "has_regressions=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate Performance Graphs
        run: |
          python3 scripts/generate_perf_graphs.py \
            --base base/ \
            --pr pr/ \
            --output perf-graphs/

      - name: Upload Performance Graphs
        uses: actions/upload-artifact@v4
        with:
          name: performance-graphs
          path: perf-graphs/

      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('comparison-report.md', 'utf8');

            // Find existing performance comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.data.find(comment =>
              comment.user.login === 'github-actions[bot]' &&
              comment.body.includes('Performance Comparison Report')
            );

            const commentBody = `## Performance Comparison Report\n\n${report}\n\n` +
              `<details>\n<summary>View Performance Graphs</summary>\n\n` +
              `![JS Benchmarks](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}/artifacts/js-benchmarks.png)\n` +
              `</details>`;

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: commentBody
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody
              });
            }

      - name: Fail on Regression
        if: steps.comparison.outputs.has_regressions == 'true'
        run: |
          echo "⚠️ Performance regressions detected!"
          cat comparison-report.md
          exit 1

      - name: Post Summary
        if: always()
        run: |
          cat comparison-report.md >> $GITHUB_STEP_SUMMARY
